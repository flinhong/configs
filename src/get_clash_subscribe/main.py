import os
import re
import time
from datetime import datetime

import requests
import yaml

requests.packages.urllib3.disable_warnings()


ok_code = [200, 201, 202, 203, 204, 205, 206]


def write_log(content, level="INFO"):

    date_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))
    update_log = f"[{date_str}] [{level}] {content}\n"
    print(update_log)
    with open(f'./public/subscribe/log/{time.strftime("%Y-%m", time.localtime(time.time()))}-update.log', 'a', encoding="utf-8") as f:
        f.write(update_log)

def get_subscribe_main():
    dirs = './public/subscribe'
    if not os.path.exists(dirs):
        os.makedirs(dirs)
    log_dir = "./public/subscribe/log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    clash_req = requests.get("https://github.com/ermaozi/get_subscribe/raw/main/subscribe/clash.yml")

    # æ£€æŸ¥ä¸‹è½½æ˜¯å¦æˆåŠŸ
    if not clash_req.status_code in ok_code:
        write_log("æ›´æ–°å¤±è´¥ï¼æ— æ³•æ‹‰å– emzclash è®¢é˜…å†…å®¹", "WARN")
        return None
    
    try:
        clash_content = check_and_validate_file("https://github.com/ermaozi/get_subscribe/raw/main/subscribe/clash.yml")
        clash_content_replaced = re.sub(r"https://raw.githubusercontent.com", "https://cdn.honglin.ac.cn/statically/gh", clash_content, flags=re.IGNORECASE)
        # clash_content_replaced = re.sub(r"http://www.gstatic.com/generate_204", "https://www.gstatic.com/generate_204", clash_content_replaced, flags=re.IGNORECASE)
        # clash_content_replaced = re.sub(r"tolerance: 50", "tolerance: 10", clash_content_replaced, flags=re.IGNORECASE)

        # å°†æ›´æ–°åçš„å†…å®¹å†™å…¥æ–‡ä»¶
        with open(dirs + '/clash.yml', 'w', encoding="utf-8") as f:
            f.write(clash_content_replaced)
            write_log(f"åŒæ­¥è®¢é˜…æˆåŠŸ", "INFO")

        extra_proxies_lb = download_extra_proxies(False)
        if extra_proxies_lb is not None:
            clash_yaml = yaml.safe_load(clash_content_replaced)
            clash_content_updated = append_proxies(clash_yaml, extra_proxies_lb, False)
        else:
            clash_content_updated = yaml.safe_load(clash_content_replaced)

        extra_proxies_auto = download_extra_proxies(True)
        if extra_proxies_auto is not None:
            clash_yaml = yaml.safe_load(clash_content_updated)
            clash_content_updated = append_proxies(clash_yaml, extra_proxies_auto, True)
        else:
            clash_content_updated = yaml.safe_load(clash_content_replaced)

        # å°†åˆå¹¶åçš„å†…å®¹å†™å…¥æ–‡ä»¶
        with open(dirs + '/clash.yml', 'w', encoding="utf-8") as f:
            f.write(clash_content_updated)
            write_log(f"åˆå¹¶è®¢é˜…æˆåŠŸ", "INFO")
    except Exception as e:
        write_log(f"è®¢é˜…è§£æé”™è¯¯", "WARN")
        return None
    
def append_proxies(clash_yaml, proxies, auto=False):
    # å°†æ–°çš„èŠ‚ç‚¹åˆ—è¡¨è¿½åŠ åˆ°åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨
    original_proxies = clash_yaml.get('proxies', [])
    # å°†åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨ä¸ node_free_proxies åˆå¹¶
    original_proxies.extend(proxies)
    # æ›´æ–°åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨
    clash_yaml['proxies'] = original_proxies

    if auto == True:
        # æ›´æ–°[â™»ï¸ è‡ªåŠ¨é€‰æ‹©]åˆ—è¡¨
        original_groups = clash_yaml.get('proxy-groups', [])
        for group in original_groups:
            if group.get('name') == 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©':
                # è·å–åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨
                original_auto_proxies = group.get('proxies', [])
                # å°†åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨ä¸é¢å¤–èŠ‚ç‚¹åˆå¹¶
                extra_proxy_names = [item['name'] for item in proxies]
                original_auto_proxies.extend(extra_proxy_names)
                # æ›´æ–°åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨
                group['proxies'] = original_auto_proxies
                group['url'] = "https://www.facebook.com/common/referer_frame.php"
                group['interval'] = 60
                group['tolerance'] = 5
        # å°†æ›´æ–°åçš„å†…å®¹å†™å›åˆ°YAMLæ–‡ä»¶
        clash_yaml_replaced = yaml.dump(clash_yaml, default_flow_style=False, allow_unicode=True)
        return clash_yaml_replaced

    if auto != True:
        # è¿½åŠ [ğŸª è´Ÿè½½å‡è¡¡]åˆ—è¡¨
        original_groups = clash_yaml.get('proxy-groups', [])
        original_groups.append({
            'name': "ğŸª è´Ÿè½½å‡è¡¡",
            'type': "load-balance",
            'url': "https://client3.google.com/generate_204",
            'interval': 120,
            'strategy': "consistent-hashing",
            'proxies': [item['name'] for item in proxies]
        })
        
        # ä¿®æ”¹åŸ[â™»ï¸ è‡ªåŠ¨é€‰æ‹©]åˆ—è¡¨
        for group in original_groups:
            if group.get('name') == 'â™»ï¸ è‡ªåŠ¨é€‰æ‹©':
                # è·å–åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨
                original_auto_proxies = group.get('proxies', [])
                # å°†è´Ÿè½½å‡è¡¡æ·»åŠ åˆ°è‡ªåŠ¨é€‰æ‹©
                original_auto_proxies.insert(0, "ğŸª è´Ÿè½½å‡è¡¡")
                # æ›´æ–°åŸè®¢é˜…çš„èŠ‚ç‚¹åˆ—è¡¨
                group['proxies'] = original_auto_proxies
                group['url'] = "https://client3.google.com/generate_204"
                group['interval'] = 60
                group['tolerance'] = 1

        clash_yaml_replaced = yaml.dump(clash_yaml, default_flow_style=False, allow_unicode=True, sort_keys=False)
        return clash_yaml_replaced

def validate_yaml(data):
    try:
        yaml.safe_load(data)
        return True
    except yaml.YAMLError as exc:
        print(f"Error validating YAML: {e}")
        return False

def check_and_validate_file(url):
    response = requests.get(url)
    print(f"è·å– {url} çŠ¶æ€ç : {response.status_code}")
    if not response.status_code in ok_code:
        return None
    
    try:
        data = response.content.decode('utf-8')
        if validate_yaml(data):
            return data
        else:
            return None
    except Exception as e:
        print(e)
        return None

def download_extra_proxies(auto=False):
    current_date = datetime.now()
    current_date_str = (current_date).strftime('%Y/%m/%Y%m%d')

    if auto == True:  
        urls = [
            # "https://raw.githubusercontent.com/zhangkaiitugithub/passcro/main/speednodes.yaml",
            # "https://raw.githubusercontent.com/NiREvil/vless/main/sub/clash-meta-wg.yml",
            # "https://raw.githubusercontent.com/ts-sf/fly/main/clash",
            # "https://raw.githubusercontent.com/MrMohebi/xray-proxy-grabber-telegram/master/collected-proxies/clash-meta/actives_under_1000ms.yaml",
            # "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge_yaml.yml",
            # "https://suo.yt/d5UCwQr", # https://github.com/Pawdroid/Free-servers
            f"https://v2rayshare.githubrowcontent.com/{current_date_str}.yaml",
            f"https://nodefree.githubrowcontent.com/{current_date_str}.yaml",
            # "https://raw.githubusercontent.com/peasoft/NoMoreWalls/master/list.yml"
        ]
    else:
        urls = [
            "https://raw.githubusercontent.com/zhangkaiitugithub/passcro/main/speednodes.yaml",
            # "https://raw.githubusercontent.com/NiREvil/vless/main/sub/clash-meta-wg.yml",
            # "https://raw.githubusercontent.com/ts-sf/fly/main/clash",
            # "https://raw.githubusercontent.com/MrMohebi/xray-proxy-grabber-telegram/master/collected-proxies/clash-meta/actives_under_1000ms.yaml",
            # "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge_yaml.yml",
            # "https://suo.yt/d5UCwQr", # https://github.com/Pawdroid/Free-servers
            # f"https://v2rayshare.githubrowcontent.com/{current_date_str}.yaml",
            # f"https://nodefree.githubrowcontent.com/{current_date_str}.yaml",
            "https://raw.githubusercontent.com/peasoft/NoMoreWalls/master/list.yml"
        ]
    all_proxies = []

    for file_url in urls:
        try:
            data = check_and_validate_file(file_url)
            if data is not None:
                proxies = get_extra_proxies(data)
                all_proxies.extend(proxies)
            
        except Exception as e:
            print(f"Error downloading or processing {file_url}: {e}")

    unique_proxies = remove_duplicate_proxies(all_proxies)
    return unique_proxies

def remove_duplicate_proxies(proxies):
    seen_servers = set()
    unique_proxies = []
    for proxy in proxies:
        name = proxy.get("name")
        server = proxy.get("server")
        key = (name, server)
        print(key)

        if key not in seen_servers:
            unique_proxies.append(proxy)
            seen_servers.add(key)

    return unique_proxies

def get_extra_proxies(data):
    yaml_data = yaml.safe_load(data)
    proxies = yaml_data.get('proxies', [])
    filter_string = ['æœªçŸ¥', 'tg', 'TG', 'KB/s']
    filtered_proxies = [proxy for proxy in proxies if not any(str in proxy['name'] for str in filter_string)]
    # filtered_proxies = [proxy for proxy in filtered_proxies if not proxy['type']=='ss' ]

    # é˜²æ­¢åç§°å’Œ ermz é‡å¤
    for proxy in filtered_proxies:
        if 'name' in proxy and proxy['name']:
            proxy['name'] = f"X_{proxy['name']}"
        else:
            print("No name found in the proxy")
            return None

    return filtered_proxies

def main():
    get_subscribe_main()


# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    main()
